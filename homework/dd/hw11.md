---
title: "hw11"
author: "Your Name"
date: "2017年5月3日"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 4.7#8

I would choose **logistic regression** to use for classification of new observations. As we know，when KNN with K=1, the training error rate would be  0% and so the an error rate in test data is 18%*2=36%,which is greater than 30% of logistic regression.


# 6.8#5

##a


$\hat \beta_0=0$ and $n=p=2$,So the Minimize:

$$(y_1 - \hat{\beta}_1x_{11} - \hat{\beta}_2x_{12})^2 + (y_2 - \hat{\beta}_1x_{21} - \hat{\beta}_2x_{22})^2 + \lambda (\hat{\beta}_1^2 + \hat{\beta}_2^2)$$

##b

Because $x_{11} = x_{12} = x_1$ and $x_{21} = x_{22} = x_2$.So 

$$\hat{\beta^*}_1 = \frac{x_1y_1 + x_2y_2 - \hat{\beta^*}_2(x_1^2 + x_2^2)}{\lambda + x_1^2 + x_2^2}$$
$$\hat{\beta^*}_2 = \frac{x_1y_1 + x_2y_2 - \hat{\beta^*}_1(x_1^2 + x_2^2)}{\lambda + x_1^2 + x_2^2}$$

Finally $$\hat{\beta^*}_1 = \hat{\beta^*}_2$$

##c

Minimize:

$$(y_1 - \hat{\beta}_1x_{11} - \hat{\beta}_2x_{12})^2 + (y_2 - \hat{\beta}_1x_{21} - \hat{\beta}_2x_{22})^2 + \lambda (| \hat{\beta}_1 | + | \hat{\beta}_2 |)$$

##d


The Lasso constraint take the form $$| \hat{\beta}_1 | + | \hat{\beta}_2 | < s$$, 
which when plotted take the familiar shape of a diamond centered at origin (0,0). Next consider the squared optimization constraint 
$$(y_1 - \hat{\beta}_1x_{11} - \hat{\beta}_2x_{12})^2 + (y_2 - \hat{\beta}_1x_{21} - \hat{\beta}_2x_{22})^2$$.


In this case,the Minimize can be simplify  to follow 
$$2.(y_1 - (\hat{\beta}_1 + \hat{\beta}_2)x_{11})^2$$


This optimization problem has a simple solution: $$\hat{\beta}_1 + \hat{\beta}_2 = \frac{y_1}{x_{11}}$$.
This is a line parallel to the edge of Lasso-diamond $$\hat{\beta}_1 + \hat{\beta}_2 = s$$.
 Now solutions to the original Lasso optimization problem are contours of the function $$(y_1 - (\hat{\beta}_1 + \hat{\beta}_2)x_{11})^2$$


Finally, as $\beta_1$ and $\beta_2$very along the line $$\hat{\beta}_1 + \hat{\beta}_2 = \frac{y_1}{x_{11}}$$.these contours touch the Lasso-diamond edge  $\hat{\beta}_1 + \hat{\beta}_2 = s$ at different points. As a result, the entire edge $\hat{\beta}_1 + \hat{\beta}_2 = s$ is a potential solution to the Lasso optimization problem.

Similar argument can be made for the opposite Lasso-diamond edge $\hat{\beta}_1 + \hat{\beta}_2 = -s$,
Thus, the Lasso problem does not have a unique solution. The general form of solution is given by two line segments: 

$$\hat{\beta}_1 + \hat{\beta}_2 = s; \hat{\beta}_1 \geq 0; \hat{\beta}_2 \geq 0 \\ \hat{\beta}_1 + \hat{\beta}_2 = -s; \hat{\beta}_1 \leq 0; \hat{\beta}_2 \leq 0$$

# 7.9#4

```{r}

yf <- function(x) {
	b1 <- ifelse(x>=0 & x <=2, 1, 0) - (x-1) * ifelse(x>=1 & x <=2,1,0)
	b2 <- (x-3) * ifelse(x>=3 & x <= 4 ,1,0) + ifelse(x>4 & x <=5,1,0)
	y <- 1 + 1 * b1 +  3 * b2
	return(y)
}
curve(yf,from = -2,to = 2)
```


# 7.9#9

##a

```{r}
library(MASS)
library(ggplot2)
data('Boston')
lm1 = lm(nox ~ poly(dis, 3), data = Boston)
summary(lm1)
ggplot(data=Boston,aes(x=dis,y=nox))+
  geom_point()+
  geom_smooth(method = 'lm',formula = y~poly(x,3),se = F)
```

##b

```{r}
rss = rep(NA, 10)
for (i in 1:10) {
    lm2 = lm(nox ~ poly(dis, i), data = Boston)
    rss[i] = sum(lm2$residuals^2)
}
rss
plot(rss,type='b',xlab='Degree',ylab='RSS')

ggplot(data=Boston,aes(x=dis,y=nox))+
  geom_point(col=gray(0.5),pch=5)+
  geom_smooth(method = 'lm',formula = y~poly(x,1),se = F,lty=1,col=1)+
  geom_smooth(method = 'lm',formula = y~poly(x,2),se = F,lty=2,col=2)+
  geom_smooth(method = 'lm',formula = y~poly(x,3),se = F,lty=3,col=3)+
  geom_smooth(method = 'lm',formula = y~poly(x,4),se = F,lty=4,col=4)+
  geom_smooth(method = 'lm',formula = y~poly(x,5),se = F,lty=5,col=5)+
  geom_smooth(method = 'lm',formula = y~poly(x,6),se = F,lty=6,col=6)+
  geom_smooth(method = 'lm',formula = y~poly(x,7),se = F,lty=7,col=7)+
  geom_smooth(method = 'lm',formula = y~poly(x,8),se = F,lty=8,col=8)+
  geom_smooth(method = 'lm',formula = y~poly(x,9),se = F,lty=9,col=9)+
  geom_smooth(method = 'lm',formula = y~poly(x,10),se = F,lty=10,col=10)

```


##c

The degree 3 has lowest cv error.So I choose 3 as the best polynomial degree.

```{r}
library(boot)
cv = c()
set.seed(2017)
for (i in 1:10) {
    lm1 = glm(nox ~ poly(dis, i), data = Boston)
    cv[i] = cv.glm(data = Boston, 
                   glmfit = lm1, K = 5)$delta[2]
}
plot(cv, 
     xlab = "Degree", 
     ylab = "CV error",
     type = "b")
which.min(cv)
```



# 8.4 #5

- Majority approach: RED.
- Average approach: GREEN.

```{r}
p <-  c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
# Majority approach
table(p >0.5)
# Average approach
mean(p) > 0.5
```

# 9.7 #3

##a

```{r}
mydf <- data.frame(Obs=1:7,
                   x1 = c(3, 2, 4, 1, 2, 4, 4),
                   x2 = c(4, 2, 4, 4, 1, 3, 1),
                   y = c("red", "red", "red", "red", "blue", "blue", "blue"))
mydf
plot(x = mydf$x1,
     y=mydf$x2,
     col=c("red", "red", "red", "red", "blue", "blue", "blue"),
     pch=19,xlab='',ylab='')

```



##b

$$b = (3.5 - 1.5) / (4 - 2) = 1 \\
a = X_2 - X_1 = 1.5 - 2 = -0.5$$

```{r}
plot(x = mydf$x1,
     y=mydf$x2,
     col=c("red", "red", "red", "red", "blue", "blue", "blue"),
     pch=19,xlab='',ylab='')
abline(-0.5, 1,col='orange',lty=2,lwd=2)
```

##c

$\beta_0=0.5$,$\beta_1=-1$,$\beta_2=1$

##d

```{r}
plot(x = mydf$x1,
     y=mydf$x2,
     col=c("red", "red", "red", "red", "blue", "blue", "blue"),
     pch=19,xlab='',ylab='')
abline(-0.5, 1,col='orange',lty=2,lwd=2)
abline(-1, 1, lty = 2,col='blue')
abline(0, 1, lty = 2,col='blue')

```

##e

```{r}
plot(x = mydf$x1,
     y=mydf$x2,
     col=c("red", "red", "red", "red", "blue", "blue", "blue"),
     pch=19,xlab='',ylab='')
abline(-0.5, 1,col='orange',lty=2,lwd=2)
arrows(2, 1, 2, 1.5)
arrows(2, 2, 2, 1.5)
arrows(4, 4, 4, 3.5)
arrows(4, 3, 4, 3.5)
```

##f

A slight movement of $7^{th}$ observation (4,1) blue would not have an effect on the maximal margin hyperplane since its movement would be outside of the margin.


##g

$$−0.6−X_1+X_2>0$$

```{r}
plot(x = mydf$x1,
     y=mydf$x2,
     col=c("red", "red", "red", "red", "blue", "blue", "blue"),
     pch=19,xlab='',ylab='')
abline(-0.6, 1,col='green',lty=2,lwd=2)
```


##h
```{r}
plot(x = mydf$x1,
     y=mydf$x2,
     col=c("red", "red", "red", "red", "blue", "blue", "blue"),
     pch=19,xlab='',ylab='')
points(x =4 , 2.5, col = 'green',pch='*',cex=3)
```


# 10.7 #4

##a

Not enough information to tell. The maximal intercluster dissimilarity could be equal or not equal to the minimial intercluster dissimilarity. If the dissimilarities were equal, they would fuse at the same height. If they were not equal, they single linkage dendogram would fuse at a lower height.

##b

They would fuse at the same height because linkage does not affect leaf-to-leaf fusion.

# 10.7 #9

##a

```{r, message=FALSE, warning=FALSE}
library(ISLR)
set.seed(2017)
hc1 <-  hclust(dist(USArrests), method="complete")
plot(hc1)
```

##b

```{r}
cutree(hc1, 3)
table(cutree(hc1, 3))
```

##c

```{r}
set.seed(2017)
hc2 <-  hclust(dist(scale(USArrests)), method="complete")
plot(hc2)
```

##d

```{r}
cutree(hc2, 3)
table(cutree(hc2, 3))
table(cutree(hc2, 3),cutree(hc1, 3))
```


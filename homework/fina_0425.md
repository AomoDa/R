---
title: "Final"
author: "Your Nmae"
date: "2017-04-25"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Q1

##a

There are some apparent linear relationships between `police`,`register` and  `weekly` with `homicide` from the scatter plots.


```{r}
x <- read.table('C://Users//mali//Documents//detroit.txt',
                header = T)
pairs(x[,-1],pch=16,col='darkred')
```

##b


- `police`,`register` and  `weekly`have statistical significant effect on the homicide rate
- `police` explains the largest percentage of variation in the homicide rate,which is 92.9%.

-----

Variable|T-test p-value| F-test p-value |Significant|Explains (R^2)
--------|----------|------------|---------|----------
police|0|0|Yes|92.9%
unemp|0.491|0.491|No|4.4%
register|0.001|0.001|Yes|66.7%
weekly|0|0|Yes|78.9%

-----


```{r}
lm1 <- lm(homicide~police,data=x)
lm2 <- lm(homicide~unemp,data=x)
lm3 <- lm(homicide~register,data=x)
lm4 <- lm(homicide~weekly,data=x)
summary(lm1)
summary(lm2)
summary(lm3)
summary(lm4)
```

##c

- Use forward stepwise to selection variables.Finally `police ` and `register` are selected. 
- `police` is first selected 
- `register` is selected after the `police` effects are
considered.
- And the largest percentage of variation in the homicide rate is 96.12%


```{r}
# forward 
step(object = lm(homicide~1,data=x),
     scope = list(upper=~police+unemp+register+weekly),
     direction = 'forward')
lm_best <- lm(formula = homicide ~ police + register, data = x)
summary(lm_best)
```

##d

- The prediction is 27.04
- 95% prediction interval for the homicide rate is $[0,65.70]$.

```{r}
predict(object = lm2,
        newdata = x[x$year==1970,],
        interval = 'prediction',
        level = 0.95)
```

##e

- Residuals plot tells me that there maybe some relation between residuals and fitted values but in this case it is acceptable.
- Q-Q Plot of residuals tells me that residuals maybe a normal distribution.

```{r}
par(mfrow=c(2,2))
plot(lm2)
```

#Q2

- The p value of Shapiro-Wilk Normality Test is 0.5422,which is not statistically significant at 0.05 level and means that the data is normal distribution.So I can use paired t-test method.
- **Null hypothesis**: true difference in means is less than 0
- **Alternative hypothesis**: true difference in means is greater than 0
- The p value of paired t-test with one sided is 0,which is very statistically significant at 0.05 level and **means that repair on  Garage Elegance is higher than Joe’s Garage**.

```{r}
mydf <- data.frame(Car = 1:15,
	Garage_Elegance=c(7.6,10.2,9.5,1.3,3.0,6.3,5.3,6.2,2.2,4.8,11.3,12.1,6.9,7.6,8.4),
	Joes_Garage=c(7.3,9.1,8.4,1.5,2.7,5.8,4.9,5.3,2.0,4.2,11.0,11.0,6.1,6.7,7.5))
mydf
# Shapiro-Wilk Normality Test
shapiro.test(c(mydf$Garage_Elegance,mydf$Joes_Garage))
# Paired t-test
# one-sided 
# null hypothesis: true difference in means is less than 0
# alternative hypothesis: true difference in means is greater than 0
with(mydf,t.test(x =Garage_Elegance,
                 y=Joes_Garage,
                 paired = T,
                 alternative = 'greater'))
```

#Q3


##a

- null hypothesis: the trend of the S&P 500 index for the rest of the year is the same as it in the month of January.
- alternative hypothesis: the trend of the S&P 500 index for the rest of the year is independent to it in the month of January.


##b

The p value with two-sided test should be multiplied by 2 and I will get the p value with the one sided test.

##c

The p value of Chisq-Test is very statistically significant at 0.05 level ,which means that I should reject null hypothesis.


```{r, message=FALSE, warning=FALSE}
library(gmodels)
mymt <- matrix(c(33,11,3,13),ncol = 2,byrow = T)
rownames(mymt) <- c('Up','Down')
colnames(mymt) <- c('Up','Down')
CrossTable(mymt,
           expected = T,
           prop.r = F,
           prop.c = F,
           prop.t = F,
           prop.chisq = F)

```

##d

if the S&P 500 index is up in the month of January, then the S&P 500 index will be up for the rest of the year. On the other hand, if it is down in January, then it will be down for the rest of the year.

##e

- if the S&P 500 index is up in the month of January,I would increase investment
- if the S&P 500 index is down in the month of January,I would reduce investment.

#Q4

```{r, message=FALSE, warning=FALSE}
library(reshape)
mt <- matrix(c(10.8,11.1,5.4,5.8,9.1,11.1,4.6,5.3,13.5,8.2,7.4,3.2,9.2,11.3,5.0,7.5),
             nrow = 4,
             byrow = F)
rownames(mt) <- c('0','1000','5000','10000')
colnames(mt) <- c('p1','p2','p3','p4')
mt
melt.mt <- melt(mt)
melt.mt
```


##a

- null hypothesis : the nematodes have no effect on the plant growth at all.
- alternative hypothesis : the nematodes have some effect on the plant growth at all.
- The p value is 0.0313,which is very statistically significant at 0.05 and means that we shold reject null hypothesis.


```{r}
# Shapiro-Wilk Normality Test
shapiro.test(melt.mt$value)
# t test
t.test(x =melt.mt$value[melt.mt$X1==0],
       y =melt.mt$value[melt.mt$X1!=0])
```

##b

- null hypothesis: the introduction of nematodes reduces the plant growth.
- alternative hypothesis: the introduction of nematodes doesn't reduce the plant growth.
- The p value is 0.9844,which is not  statistically significant at 0.05 and means that we cann't reject null hypothesis.


```{r}
t.test(x =melt.mt$value[melt.mt$X1==0],
       y =melt.mt$value[melt.mt$X1!=0],
       alternative = 'less')
```


##c

t test with bonferroni adjusted should be used.

- the two groups are treated with 1000 nematodes per plant and the group treated with 5000 nematodes per
plant.
- null hypothesis: two groups do have the same plant growth
- alternative hypothesis: two groups do have different plant growth.
- The p value is 0.0089,which is very  statistically significant at 0.05 and means that we should reject null hypothesis.

```{r}
pairwise.t.test(x = melt.mt$value,
                g = melt.mt$X1,
                p.adjust.method = 'bonferroni')
```




##b


##c



#Q5

```{r}
mydf1 <- data.frame(months=c(14,29,6,25,18,4,18,12,22,6,30,11,30,5,20,13,9,32,24,13,19,4,28,22,8,14),
                    success=c(0,0,0,1,1,0,0,0, 1,0,1,0,1,0,1,0,0,1,0,1,0,0,1,1,1,0))
mydf1
```

##a

- The p value of Shapiro-Wilk Normality Test is 0.1511,which is not  statistically significant at 0.05 level and means that the data is normal distribution.So I can use  t-test method.
- The mean of work experience whose task was successfully completed is 22.5 months.But the means of  work experience whose task was not successfully completed is 12.5 months.
- The p value of Two Sample t-test is 0.003,which is statistically significant at 0.05 level and means that work experience improve the programmer’s ability.

```{r}
#Shapiro-Wilk Normality Test
shapiro.test(mydf1$months)
# Welch Two Sample t-test
t.test(months~success,data=mydf1)

```

##b

The 95% confidence estimation interval of the improvement in the odds of completing the task with specified time period for each extra year of work experience is $[0.05,0.32]$.

```{r}
glm1 <- glm(success~months,
            data=mydf1,
            family =binomial(link = "logit") )
confint(glm1,level = 0.95)
```

##c

- $61624.92 per year should the employer pay a programmer with 24 months of previous work experience.
- $39992.59 per year should the employer pay a programmer with 24 months of previous work experience.
- If a second programmer with 18 months of work
experience is willing to accept a yearly salary of $(X-10,000),the second programmer is a better deal for the company according to the analysis,because 10,000 is far less than 39992.59.

```{r}
# 24 months of previous work experience
(p24 <- predict.glm(object = glm1,
                    newdata =data.frame(months=24) ,
                    type='response'))
(p24*90000)
# 18 months of previous work experience
(p18 <- predict.glm(object = glm1,
                    newdata =data.frame(months=18) ,
                    type='response'))
(p18*90000)


```


#Q6

I will use Pearson's product-moment correlation.

- null hypothesis: true correlation is equal to 0
- alternative hypothesis: true correlation is not equal to 0

If I only know the the sample correlation of $X$ and $Y$ is 0.6,I cann't do hypothesis test.

But if I also know that the sample size is
40 (pairs of X and Y),I can do hypothesis test.

$$Z_r=\frac{1}{2}log(\frac{1+r}{1-r})=0.6931$$  

$$Z=\frac{Z_r}{\sqrt{\frac{1}{n-3}}}=4.2163$$. 

$$p-value=0$$
So the p value is 0 which is  very statistically significant at 0.05 level and means in this data set with 40 sample size we should reject $H_0$ .



```{r}
# Pearson's product-moment correlation
# null hypothesis: true correlation is  equal to 0
# alternative hypothesis: true correlation is not equal to 0
zr <- (log( (1+0.6)/(1-0.6))/2)
z <- zr/sqrt(1/(40-3))
# two sided p value 
(1-pnorm(z))*2
```


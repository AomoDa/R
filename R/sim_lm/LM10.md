---
title: "Untitled"
author: "Your Name"
date: "2016-11-09"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



##Abstract

- 自己写一下
- 自己写一下
- 自己写一下
- 自己写一下
- 自己写一下

## import data 

首先将数据导入到R语言中并且加载本文分析需要用到的包，包括 "car" 和 "ggplot2"。导入数据之后，一共两个变量，分别为响应变量$y$和自变量$x$。

- 响应变量$y$的最小值为-1568.99，最大值为-45.76，均值为-772.35，中位数为-753.59。
- 自变量$x$的最小值为11.30，最大值为112.05，均值为52.04，中位数为49.48。
- 相关系数$COR(X,Y)=-0.8968802$，说明$y$和$x$高度负相关，$y$随着$x$的增大而减小。

```{r, message=FALSE, warning=FALSE}
library(car)
library(ggplot2)
mydata <- read.csv('C://Users//AomoDa//Documents//MidtermData10(HENRY).csv',header = T)
str(mydata)
summary(mydata)
cor(mydata)
```


##EDA

接下来进行EDA部分，将会探索数据之间的关系。


###boxplot

```{r}
par(mfrow=c(1,2))
boxplot(mydata$x,main='boxplot of x',col=3)
boxplot(mydata$y,main='boxplot of x',col=4)
par(mfrow=c(1,1))
```

###histogram  and qqplot

分析结果如下：

- 从histogram看$x$的分布有一个波峰，从Q-Q图看$x$似乎是服从正态分布的。Shapiro-Wilk normality test的结果显示$W=0.96876$,$p-value = 0.07755>0.05$，因此可以认为$x$ 服从正态分布。

- 从histogram看$y$的分布有一个波峰，比$x$更接近正态分布，从Q-Q图也可以得出相同的结论的。Shapiro-Wilk normality test的结果显示$W=0.97018$,$p-value = 0.09334>0.05$，因此可以认为$x$ 服从正态分布。

```{r}
par(mfrow=c(1,2))
hist(mydata$x,xlab='x',freq=F)
lines(density(mydata$x),col='red',lty=2)
qqPlot(mydata$x,main='qqplot of y')
par(mfrow=c(1,1))

par(mfrow=c(1,2))
hist(mydata$y,xlab='y',freq=F)
lines(density(mydata$y),col='red',lty=2)
qqPlot(mydata$y,main='qqplot of y')
par(mfrow=c(1,1))

##normal text
shapiro.test(mydata$x)
shapiro.test(mydata$y)
```

### Linear Analysis

以下绘制$x$ 和$y$的散点图，并且进行了一次项拟合和多次项拟合，分析结论如下：

- 从第一幅图看，一次项的直线除个别点之外，全部均值的分布在直线周围，并且直线似乎要穿过坐标轴的$(0,0)$点，意外着方程的截距可能是0.
- 第二幅图是二次项的拟合，比一次项光滑了，甚至可以很好的解释一次项目不能解释的某些点。
- 第三幅图是三次项的拟合，比二次项更佳光滑了，图中所有的点全部均匀的分布在曲线周围。
- 因此接下来的建模分析中，我将重点分析一到三次项的回归方程。


```{r}
## plot 
ggplot(data=mydata,aes(x,y))+geom_point()+    
      geom_smooth(method = 'lm',col='blue')+
      labs(title='y~x')
ggplot(data=mydata,aes(x,y))+geom_point()+
    geom_smooth(col='red',formula = y~x+I(x^2),method = 'lm')+
    labs(title='y~x+x^2')
ggplot(data=mydata,aes(x,y))+geom_point()+
    geom_smooth(col='orange',formula = y~x+I(x^2)+I(x^3),method = 'lm')+
    labs(title='y~x+x^2+x^3')
```


## MODEL





### Best Model

根据上文的分析，接下来我将建立四个模型并且从中选取最优的一个。

- $lm0$，一次项含截距，分析发现截距项目的T检验不显著。
- $lm1$，一次项不含截距，全部显著。
- $lm2$，二次项不含截距，全部显著,但是一次项和二次项之间存在多重共线性问题，使得方程系数估计不精确。
- $lm3$，三次项不含截距，全部显著,但是一次项、二次项和三次项之间存在多重共线性问题，使得方程系数估计不精确。
- 综合考虑，我选择$lm1$为我的模型，其中$R_{adj}^2=0.9611$，方程拟合非常好。


```{r, warning=FALSE}
lm0 <- lm(y~x,data=mydata)
lm1 <- lm(y~x-1,data=mydata)
lm2 <- lm(y~I(x)+I(x^2)-1,data=mydata)
lm3 <- lm(y~I(x)+I(x^2)+I(x^3)-1,data=mydata)
vif(lm2)
summary(lm1)
```


###MODEL diagnostics

- 从线性回归诊断图中发现，模型残差除个别几个点之前，其他的点全部靠拢在"Q-Q line" 附近，可以判断残差近似服从正态分布，符合线性回归的综合假设。
- 对$lm1$的残差进行Box-Ljung test，计算得到$X-squared = 1.2431$,$p-value = 0.2649>0.05$,因此残差之间不存在自相关，由于acf图同样可以得出此结论。
- 从杠杆图中发现第61个数据点为强影响点，绘制Regression Influence Plot发现 第61个点对模型参数估计影响很大，其中$StudRes=6.756897$,$Hat=0.0538806$,$CookD=1.57848$。可以考虑将此数据点删除再估计线性回归系数。



```{r}
par(mfrow=c(2,2))
plot(lm1)
par(mfrow=c(1,1))
#Box.test
Box.test(residuals(lm1),type = 'Ljung')
#acf
acf(residuals(lm1))
#Regression Influence Plot
influencePlot(lm1)
```

##discuss

- 分析发现第61个数据点为强影响点，如果删除的话影响数据的真实性，不删除影响模型的准确性。
- 是否需要考虑非线性模型，是否非线性模型会比线性模型更好？

##conclusion

根据结果，分析得出结论为:$$Y=-14.5803 X + \epsilon$$,其中$\epsilon$为残差项。

##Reference

- 自己写一下
- 自己写一下
- 自己写一下
- 自己写一下
- 自己写一下

##appendix : R code

```{r, eval=FALSE, include=T}
# lm
# import data 
library(car)
library(ggplot2)
mydata <- read.csv('C://Users//AomoDa//Documents//MidtermData10(HENRY).csv',header = T)
str(mydata)
summary(mydata)
cor(mydata)

#----------------------------------------------------------------------
##EDA
#----------------------------------------------------------------------

par(mfrow=c(1,2))
boxplot(mydata$x,main='boxplot of x',col=3)
boxplot(mydata$y,main='boxplot of x',col=4)
par(mfrow=c(1,1))

##histogram 
par(mfrow=c(1,2))
hist(mydata$x,xlab='x',freq=F)
lines(density(mydata$x),col='red',lty=2)
qqPlot(mydata$x,main='qqplot of y')
par(mfrow=c(1,1))

par(mfrow=c(1,2))
hist(mydata$y,xlab='y',freq=F)
lines(density(mydata$y),col='red',lty=2)
qqPlot(mydata$y,main='qqplot of y')
par(mfrow=c(1,1))

##normal text
shapiro.test(mydata$x)
shapiro.test(mydata$y)

## plot 


ggplot(data=mydata,aes(x,y))+geom_point()+    
      geom_smooth(method = 'lm',col='blue')+
      labs(title='y~x')

ggplot(data=mydata,aes(x,y))+geom_point()+
    geom_smooth(col='red',formula = y~x+I(x^2),method = 'lm')+
    labs(title='y~x+x^2')

ggplot(data=mydata,aes(x,y))+geom_point()+
    geom_smooth(col='orange',formula = y~x+I(x^2)+I(x^3),method = 'lm')+
    labs(title='y~x+x^2+x^3')



#----------------------------------------------------------------------
# MODEL
#----------------------------------------------------------------------

lm0 <- lm(y~x,data=mydata)
lm1 <- lm(y~x-1,data=mydata)
lm2 <- lm(y~x+I(x^2)-1,data=mydata)



#----------------------------------------------------------------------
# MODEL diagnostics
#----------------------------------------------------------------------
vif(lm2)
summary(lm1)

par(mfrow=c(2,2))
plot(lm1)
par(mfrow=c(1,1))

#Box.test
Box.test(residuals(lm1),type = 'Ljung')

#Regression Influence Plot
influencePlot(lm1)


```

